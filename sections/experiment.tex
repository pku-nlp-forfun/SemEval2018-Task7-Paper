\section{Experiment}
\label{sec:experiment}

In the previous section, we mentioned many experiments on the different corpus, embedding tools and model. We will list the performance of each attempt and highlight the best performance on both tasks.

Because the TOPIC class in subtask 1.1 is unbalanced. So we can see the phenomenon that subtask 1.2 will overall be higher than subtask 1.1.

\subsection{Different Corpus and Embedding Model}
\label{sec:different_corpus_and_embedding_model}

Word embedding almost the important thing in NLP task. So we take some word embedding method and train dataSet to tune the effect of word embedding.~\ref{tab:embedding}

First of all, the train dataSet of competition is so small that the effect of training word embedding is bad. So we import some outer dataSet to optimization the effect of word embedding. The domain of our task is about scientific papers. So, we load dataset on Citation Network Dataset. We test dblp v5, dblp v10, and acm v9, we find dblp v10 have best f1 score both in trainSet and testSet.

We also do some work on different word embedding methods, like word2vec, Bert, fastText. FatsText do the best job in our experiment. Bert doesn't have a good effect on our task. We think it may be caused by the embedding size of the pre-train model.

\input{figures/embedding_comp.tex}

\subsection{Different Feature}
\label{sec:different_feature}

We also do some work on feature engineering.~\ref{tab:feature} We test the effect of different '[PAD]' position. Clustering is also one way to improve our model. We add both one-hot and word embedding of middle words between two entities to feature lists. And we also do some artificial features like have including '\_' in middle words between two entities. We do some experiment which one by one add features to model.

\input{figures/feature_comp.tex}

\subsection{Different Model}
\label{sec:different_model}

Except for linear regression model, we also do some experiment on SVM, LinerSVC,DecisionTreeClassifier , TextCNN. \ref{tab:model}

\begin{enumerate}
    \item Linear Regression use L2-regularized logistic regression type, cost=0.05, epsilon=0.1
    \item SVM use L2-penalty, squared hinge as loss function, C=1.0, OVR on multi-classification
    \item Decision Tree use gini impurity as criterion, max depth=$\infty$, min sample split=2
    \item TextCNN use 128 filter, filter size=[6,7,8], embedding size=300, learning rate = 0.0003, batch size = 64, decay step=1000, train around 300epochs, we get a local optimum train f1 score.
\end{enumerate}

\input{figures/model_comp.tex}

\subsection{Imbalance Data}
\label{sec:impalance_data}

The train dataset is an obvious imbalance set in subtask 1.1 Topic class. So we over-sampling the train set. We, in turn, add subtask 1.2 Train Topic, subtask 1.2 Test topic, subtask 1.2 Train, subtask 1.2 Test, subtask 1.2 to subtask1.1. And test the effect of over-sampling. \ref{tab:data_imbalance}

\input{figures/imbalance_data.tex}
